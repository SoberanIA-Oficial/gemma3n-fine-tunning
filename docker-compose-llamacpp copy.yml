version: '3.8'

services:
  llama-cpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-cpp-server
    ports:
      - "8001:8001"
    volumes:
      - ./models:/models
    command: >
      -m /models/enem-gemma-3n-E2B-lora-8.Q8_0.gguf
      --port 8001
      --host 0.0.0.0
      -n 512
    restart: unless-stopped
    networks:
      - llama-net

networks:
  llama-net:
    driver: bridge